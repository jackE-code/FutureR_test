{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d434c64e",
   "metadata": {},
   "source": [
    "# Dataset:\n",
    "You will be working with two datasets: Train and Test, both containing images of 40 individuals. Each dataset has been preprocessed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5a47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb82dc",
   "metadata": {},
   "source": [
    "# Create PCA function that takes two inputs: dataset, and k= number of principle component, and return the transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6edc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca(Data,k):\n",
    "    \"Build this function\"\n",
    "    # Calculate the mean of the data\n",
    "    mean = np.mean(Data, axis=0)\n",
    "    \n",
    "    # Center the data by subtracting the mean\n",
    "    centered_data = Data - mean\n",
    "    \n",
    "    # Compute the covariance matrix of the centered data\n",
    "    covariance_matrix = np.cov(centered_data, rowvar=False)\n",
    "    \n",
    "    # Perform eigendecomposition of the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    \n",
    "    # Sort the eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    \n",
    "    # Take the top k eigenvectors\n",
    "    top_k_eigenvectors = eigenvectors[:, :k]\n",
    "    \n",
    "    # Project the centered data onto the top k eigenvectors to get the transformed data\n",
    "    transformed_data = np.dot(centered_data, top_k_eigenvectors)\n",
    "    \n",
    "    return transformed_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e39036",
   "metadata": {},
   "source": [
    "# Read your TrainData and Test data. Try to remove the last column of the training data and assign it to one variable, do same thing for the TestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be5817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the code here\n",
    "# Load your training and test data into dataframes\n",
    "train_data = pd.read_csv('TrainData.csv')\n",
    "test_data = pd.read_csv('TestData.csv')\n",
    "\n",
    "# Remove and assign the last column of the training data to a variable\n",
    "train_last_column = train_data.iloc[:, -1]\n",
    "train_data = train_data.iloc[:, :-1]\n",
    "\n",
    "# Remove and assign the last column of the test data to a variable\n",
    "test_last_column = test_data.iloc[:, -1]\n",
    "test_data = test_data.iloc[:, :-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d8316d",
   "metadata": {},
   "source": [
    "# Apply my_pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1ef819",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-829ec51a0a27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load your training dataset of images as a matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TrainData.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Define the number of PCA components you want to keep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;31m# Try a pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[0;32m    446\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[0;32m    447\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "# write the code here\n",
    "\n",
    "\n",
    "# Load your training dataset of images as a matrix\n",
    "train_data = np.load(\"TrainData.csv\")\n",
    "\n",
    "# Define the number of PCA components you want to keep\n",
    "n_components = 50  # Adjust this value as needed\n",
    "\n",
    "# Instantiate and apply PCA from your custom module\n",
    "pca = custom_pca.PCA(n_components=n_components)\n",
    "transformed_data = pca.fit_transform(train_data)\n",
    "\n",
    "print(\"Original data shape:\", train_data.shape)\n",
    "print(\"Transformed data shape:\", transformed_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f4effc",
   "metadata": {},
   "source": [
    "# Apply pca using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c0e7685",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reshaped_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0b1529b84f1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscaled_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreshaped_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'reshaped_images' is not defined"
     ]
    }
   ],
   "source": [
    "# write the code here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_images = scaler.fit_transform(reshaped_images)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 100  # Choose the number of components to keep\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(scaled_images)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "reduced_images = pca.transform(scaled_images)\n",
    "reconstructed_images = pca.inverse_transform(reduced_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310d07d",
   "metadata": {},
   "source": [
    "# Create kernel PCA function that takes two inputs: dataset, and k= number of principle component, and return the transform data. In addition, you need to create three other function one for rbf_kernel, one for polynomial_kernel, and one for linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c19afe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def rbf_kernel(x, y, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Radial Basis Function (RBF) kernel.\n",
    "    Build this function\n",
    "    \"\"\"\n",
    "    return np.exp(-gamma * np.linalg.norm(x - y) ** 2) np.exp(-gamma * np.linalg.norm(x - y) ** 2) \n",
    "\n",
    "def poly_kernel(x, y, degree=3):\n",
    "    \"\"\"\n",
    "    Polynomial kernel.\n",
    "    Build this function\n",
    "    \"\"\"\n",
    "    return (np.dot(x, y) + 1) ** degree\n",
    "\n",
    "def linear_kernel(x, y):\n",
    "    \"\"\"\n",
    "    Linear kernel.\n",
    "    Build this function\n",
    "    \"\"\"\n",
    "    return np.dot(x, y)\n",
    "\n",
    "def my_kpca(data, n_components, kernel_type='rbf', kernel_param=1.0):\n",
    "    \"\"\"\n",
    "    Kernel Principal Component Analysis (KPCA) function.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Input data as an ndarray of shape (n_samples, n_features).\n",
    "    - n_components: Number of principal components to retain.\n",
    "    - kernel_type: Type of kernel ('rbf', 'poly', or 'linear').\n",
    "    - kernel_param: Kernel parameter (e.g., gamma for RBF, degree for polynomial).\n",
    "\n",
    "    Returns:\n",
    "    - Transformed data in the KPCA space.\n",
    "    \"\"\"\n",
    "    \n",
    "    \" Build this function\"\n",
    "     n_samples = data.shape[0]\n",
    "    kernel_matrix = np.zeros((n_samples, n_samples))\n",
    "\n",
    "    # Compute the kernel matrix\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            if kernel_type == 'rbf':\n",
    "                kernel_matrix[i, j] = rbf_kernel(data[i], data[j], gamma=kernel_param)\n",
    "            elif kernel_type == 'poly':\n",
    "                kernel_matrix[i, j] = poly_kernel(data[i], data[j], degree=kernel_param)\n",
    "            elif kernel_type == 'linear':\n",
    "                kernel_matrix[i, j] = linear_kernel(data[i], data[j])\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported kernel type\")\n",
    "\n",
    "    # Center the kernel matrix\n",
    "    one_n = np.ones((n_samples, n_samples)) / n_samples\n",
    "    kernel_matrix -= one_n.dot(kernel_matrix) - kernel_matrix.dot(one_n).dot(one_n)\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors of the centered kernel matrix\n",
    "    eigvals, eigvecs = np.linalg.eigh(kernel_matrix)\n",
    "    eigvecs = eigvecs[:, ::-1]  # Sort eigenvectors in descending order of eigenvalues\n",
    "\n",
    "    # Select the top n_components eigenvectors\n",
    "    eigvecs = eigvecs[:, :n_components]\n",
    "\n",
    "    # Transform the data into KPCA space\n",
    "    kpca_data = np.dot(kernel_matrix, eigvecs)\n",
    "\n",
    "    return kpca_data\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e287808",
   "metadata": {},
   "source": [
    "# Apply my_kpca on the Train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4180ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already defined the my_kpca function and the kernel functions (rbf_kernel, poly_kernel, linear_kernel)\n",
    "\n",
    "# Load TrainData.csv\n",
    "train_data = np.genfromtxt('TrainData.csv', delimiter=',')\n",
    "\n",
    "# Extract features (assuming the features are in columns 1 to second-to-last column)\n",
    "features = train_data[:, :-1]\n",
    "\n",
    "# Apply KPCA with, for example, k=10 using the RBF kernel\n",
    "k = 10\n",
    "transformed_data = my_kpca(features, n_components=k, kernel_type='rbf', kernel_param=1.0)\n",
    "\n",
    "# Display the shape of the transformed data\n",
    "print(f\"Original data shape: {features.shape}\")\n",
    "print(f\"Transformed data shape: {transformed_data.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06316d0",
   "metadata": {},
   "source": [
    "# Apply Kpca using sklearn on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a63724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your code here\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Load TrainData.csv\n",
    "train_data = np.genfromtxt('TrainData.csv', delimiter=',')\n",
    "\n",
    "# Extract features (assuming the features are in columns 1 to second-to-last column)\n",
    "features = train_data[:, :-1]\n",
    "\n",
    "# Handle missing values (replace NaN with the mean value)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "# Flatten each image into a one-dimensional array\n",
    "flattened_images = features_imputed.reshape(features_imputed.shape[0], -1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(flattened_images)\n",
    "\n",
    "# Apply Kernel PCA with RBF kernel\n",
    "kpca = KernelPCA(n_components=5, kernel='rbf', gamma=1.0)\n",
    "transformed_data = kpca.fit_transform(features_standardized)\n",
    "\n",
    "# Display the shape of the transformed data\n",
    "print(f\"Original data shape: {flattened_images.shape}\")\n",
    "print(f\"Transformed data shape: {transformed_data.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc14d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your code her \n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "\n",
    "n_components = 50\n",
    "\n",
    "# Specify the kernel type and kernel parameter (e.g., RBF kernel with gamma=1.0)\n",
    "kernel_type = 'rbf'\n",
    "kernel_param = 1.0\n",
    "\n",
    "# Create a KernelPCA instance with the specified parameters\n",
    "kpca = KernelPCA(n_components=n_components, kernel=kernel_type, gamma=kernel_param)\n",
    "\n",
    "# Fit the KPCA model to the training data and transform it\n",
    "transformed_train_data = kpca.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e956f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb830358",
   "metadata": {},
   "source": [
    "# You can use the functions below as your classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0b009f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate distance between two points\n",
    "def dis(x1, x2):\n",
    "    return np.linalg.norm(x1 - x2)\n",
    "\n",
    "# Function to perform classification \n",
    "def myclassifier(Train, Trainlabel, Test):\n",
    "    \" Train is the training data\"\n",
    "    \" Trainlabel is the training labels\"\n",
    "    \" Test is the testing data\"\n",
    "    pred = []\n",
    "\n",
    "    for testpoint in Test:\n",
    "        pred_dis = []\n",
    "        for trainpoint in Train:\n",
    "            pred_dis.append(dis(testpoint, trainpoint))\n",
    "\n",
    "        pred.append(np.argmin(pred_dis))  \n",
    "\n",
    "    return np.array(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62885a8f",
   "metadata": {},
   "source": [
    "# Below function is to calculte the accuracy , you can use this function to get the accuracy of pca and kpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25b0c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    # Ensure that the true labels and predicted labels have the same length\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"Length of true_labels and predicted_labels must be the same.\")\n",
    "\n",
    "    # Count the number of correct predictions\n",
    "    correct_predictions = sum(1 for true, predicted in zip(true_labels, predicted_labels) if true == predicted)\n",
    "\n",
    "    # Calculate accuracy as the ratio of correct predictions to total predictions\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648d942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7d044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417279d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5456cdb0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4eec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
